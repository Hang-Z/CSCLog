{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9480f1c8-551b-4027-8666-b8da1e6e41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "\n",
    "import dateutil.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a5f2da0-8d5d-40f0-ac0a-87bba4161c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(tmp_path, encoder='one_hot'):\n",
    "    assert encoder in ['one_hot', 'semantic'], \"encoder must be one_hot or semantic\"\n",
    "    if encoder == 'one_hot':\n",
    "        one_datas = pd.read_csv(tmp_path + '.log_templates.csv', engine='c', na_filter=False, memory_map=True)\n",
    "        etype_nums = len(one_datas)\n",
    "        mapping = {etype:torch.nn.functional.one_hot(torch.tensor(idx),\\\n",
    "                                                     num_classes=etype_nums).tolist()\\\n",
    "                   for idx, etype in enumerate(one_datas['EventId'])}\n",
    "        save_js = json.dumps(mapping)\n",
    "        f2 = open(tmp_path + '_one_hot.json', 'w')\n",
    "        f2.write(save_js)\n",
    "        f2.close()\n",
    "    print('encoder done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26c9d929-3e78-4c1d-b384-840b291d57bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(log_file, window='session',window_size=0):\n",
    "    assert window == 'session', \"Only window=session is supported for HDFS dataset.\"\n",
    "    print('Loading', log_file)\n",
    "    parsed_log = pd.read_csv(log_file + '.log_structured.csv', engine='c', na_filter=False, memory_map=True, dtype={1:\"string\", 2:\"string\"})\n",
    "    \n",
    "    data_dict = OrderedDict()\n",
    "    for idx, row in parsed_log.iterrows():\n",
    "        session_list = re.findall(r'(blk_-?\\d+)', row['Content'])\n",
    "        session_set = set(session_list)\n",
    "        cur_time = row['Date'] + ' ' + row['Time']\n",
    "        for session_id in session_set:\n",
    "            if not session_id in data_dict:\n",
    "                data_dict[session_id] = []\n",
    "            # .append((feature1, feature2, ...)), here you can add more features which you need\n",
    "            data_dict[session_id].append((row['EventId'], row['Component'], cur_time))\n",
    "        last_time = cur_time\n",
    "    data_df = pd.DataFrame(list(data_dict.items()), columns=['BlockId', 'EventSequence'])\n",
    "    # data_df.to_csv(\"result/HDFS_sequence.csv\",index=None)\n",
    "    data_df.to_csv(log_file + \"_sequence.csv\",index=None)\n",
    "    # save component\n",
    "    component_set = list(set(parsed_log['Component']))\n",
    "    mapping = {component_set[i]:i for i in range(len(component_set))}\n",
    "    save_js = json.dumps(mapping)\n",
    "    f = open(log_file + '_component.json', 'w')\n",
    "    f.write(save_js)\n",
    "    f.close()\n",
    "\n",
    "def split_datas(seq_file, used_name, label_file, alpth = 0.8):\n",
    "    '''\n",
    "    split normal logs and abnormal logs by alpth rate and cat\n",
    "    '''\n",
    "    print('Loading Log: {} , label: {} and split datas by {} rate'.format(seq_file, label_file, alpth))\n",
    "    datas = pd.read_csv(seq_file + '_sequence.csv', engine='c', na_filter=False, memory_map=True)\n",
    "    labels = pd.read_csv(label_file, engine='c', na_filter=False, memory_map=True)\n",
    "    \n",
    "    normal_, anomaly_ = list(labels[labels['Label'] == 'Normal']['BlockId']), list(labels[labels['Label'] == 'Anomaly']['BlockId'])\n",
    "    normal_data, anomaly_data = datas[datas['BlockId'].isin(normal_)], datas[datas['BlockId'].isin(anomaly_)]\n",
    "    norlen, anolen = len(normal_data), len(anomaly_data)\n",
    "    print('get normal data {}, anomaly data {}'.format(norlen, anolen))\n",
    "    normal_data['Label'] = 0\n",
    "    anomaly_data['Label'] = 1\n",
    "    \n",
    "    if not os.path.exists(used_name):\n",
    "        os.mkdir(used_name)\n",
    "    \n",
    "    # split normal\n",
    "    train_test = round(norlen*alpth)\n",
    "    train_val = round(norlen*(alpth - 0.1))\n",
    "    \n",
    "    normal_data.iloc[:train_val].to_csv(used_name + 'train_normal.csv', index=None)\n",
    "    normal_data.iloc[train_val:train_test].to_csv(used_name + 'val_normal.csv', index=None)\n",
    "    normal_data.iloc[train_test:].to_csv(used_name + 'test_normal.csv', index=None)\n",
    "    \n",
    "    # split anomaly\n",
    "    train_test = round(anolen*alpth)\n",
    "    train_val = round(anolen*(alpth - 0.1))\n",
    "    \n",
    "    anomaly_data.iloc[:train_val].to_csv(used_name + 'train_anomaly.csv', index=None)\n",
    "    anomaly_data.iloc[train_val:train_test].to_csv(used_name + 'val_anomaly.csv', index=None)\n",
    "    anomaly_data.iloc[train_test:].to_csv(used_name + 'test_anomaly.csv', index=None)\n",
    "    \n",
    "    print('new file in used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b56a267-e8a5-4dea-be62-0a5f0565874e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/HDFS/result_logRel/HDFS_30s\n",
      "Loading Log: ../data/HDFS/result_logRel/HDFS_30s , label: ../data/HDFS/anomaly_label.csv and split datas by 0.8 rate\n",
      "get normal data 20022, anomaly data 2001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22922/272179234.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  normal_data['Label'] = 0\n",
      "/tmp/ipykernel_22922/272179234.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  anomaly_data['Label'] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new file in used\n",
      "encoder done\n"
     ]
    }
   ],
   "source": [
    "name = '../data/HDFS/result_logRel/HDFS_30s'\n",
    "used_name = '../data/HDFS/used_logRel_30s/'\n",
    "sample(name, window='session')\n",
    "split_datas(name, used_name, '../data/HDFS/anomaly_label.csv')\n",
    "encoder(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c47ce0-46fa-4214-9c55-0c646dd8382c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
